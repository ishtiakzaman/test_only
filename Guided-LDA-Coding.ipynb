{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9121a41",
   "metadata": {},
   "source": [
    "Step 1: Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a2961",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "string = open('Fashion_Tweets.txt', encoding='utf-8').read()\n",
    "new_str = re.sub(r'#[a-zA-Z0-9_]+', ' ', string)\n",
    "open('Fashion_Tweets_RemovedHashtags.txt', 'w', encoding='utf-8').write(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24cda35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "string = open('Fashion_Tweets_RemovedHashtags.txt', encoding='utf-8').read()\n",
    "new_str = re.sub(r'@[a-zA-Z0-9_]+', ' ', string)\n",
    "open('Fashion_Tweets_RemovedHandles.txt', 'w', encoding='utf-8').write(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "string = open('Fashion_Tweets_RemovedHandles.txt', encoding='utf-8').read()\n",
    "new_str= re.sub(r'http\\S+', '', string)\n",
    "open('Fashion_Tweets_RemovedURLS.txt', 'w', encoding='utf-8').write(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4aa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "string = open('Fashion_Tweets_RemovedURLS.txt', encoding='utf-8').read()\n",
    "new_str = re.sub('[^a-zA-Z0-9\\n\\.]', ' ', string)\n",
    "open('Fashion_Tweets_Cleaned.txt', 'w', encoding='utf-8').write(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a471515",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\De\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\De\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from lda import guidedlda as glda\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c805f8",
   "metadata": {},
   "source": [
    "Step 2: Loading the data\n",
    "Note: The text file was copied into a csv file titled Beauty_Tweers_Cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cdf92456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNO</th>\n",
       "      <th>Contents_Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>we can change our gene expression. new researc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>add garlic to your diet. the compound allicin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>baking low carb recipes with my mother via</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>weight not budging on the scale don t forget e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>happy new year to each of you remember small s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SNO                                    Contents_Processed\n",
       "0       0  we can change our gene expression. new researc...\n",
       "1       1  add garlic to your diet. the compound allicin ...\n",
       "2       2         baking low carb recipes with my mother via\n",
       "3       3  weight not budging on the scale don t forget e...\n",
       "4       4  happy new year to each of you remember small s..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1=pd.read_csv(\"C:/Users/De/Downloads/Python Files/IM/IMTweetsForPython.csv\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af585328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\De\\AppData\\Local\\Temp/ipykernel_18564/2741301449.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df1['Contents_Processed'] =df1['Contents_Processed'].str.replace(\"[^a-zA-Z#]\", \" \")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNO</th>\n",
       "      <th>Contents_Processed</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>we can change our gene expression  new researc...</td>\n",
       "      <td>change gene expression new research shows jogg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>add garlic to your diet  the compound allicin ...</td>\n",
       "      <td>garlic diet compound allicin garlic potent ant...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>baking low carb recipes with my mother via</td>\n",
       "      <td>baking low carb recipes mother</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>weight not budging on the scale don t forget e...</td>\n",
       "      <td>weight budging scale forget exercise promotes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>happy new year to each of you remember small s...</td>\n",
       "      <td>happy new year small steps success</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SNO                                    Contents_Processed  \\\n",
       "0       0  we can change our gene expression  new researc...   \n",
       "1       1  add garlic to your diet  the compound allicin ...   \n",
       "2       2         baking low carb recipes with my mother via   \n",
       "3       3  weight not budging on the scale don t forget e...   \n",
       "4       4  happy new year to each of you remember small s...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  change gene expression new research shows jogg...  \n",
       "1  garlic diet compound allicin garlic potent ant...  \n",
       "2                     baking low carb recipes mother  \n",
       "3  weight budging scale forget exercise promotes ...  \n",
       "4                 happy new year small steps success  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['Contents_Processed'] =df1['Contents_Processed'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "\n",
    "stopwords_list = stopwords.words('english')\n",
    "punctuations = list(set(string.punctuation))\n",
    "\n",
    "unwanted_list=punctuations+stopwords_list\n",
    "\n",
    "def clean_text_initial(Contents_Processed):\n",
    "    Contents_Processed = ' '.join([x.lower() for x in word_tokenize(Contents_Processed) if x.lower() not in unwanted_list and len(x)>1])\n",
    "    Contents_Processed = ' '.join([x.lower() for x in word_tokenize(Contents_Processed) if nltk.pos_tag([x])[0][1].startswith(\"NN\") or nltk.pos_tag([x])[0][1].startswith(\"JJ\")])\n",
    "    return Contents_Processed.strip()\n",
    "\n",
    "df1[\"clean_text\"]=df1.Contents_Processed.apply(lambda Contents_Processed:clean_text_initial(str(Contents_Processed)))\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfd6f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=df1.clean_text.tolist()\n",
    "vocab=list(set(word_tokenize(\" \".join(df1.clean_text))))\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1),vocabulary=vocab)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "word2id=vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "900cff79",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lda._lda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_10788/3378359881.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mguidedlda\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGuidedLDA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\guidedlda\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mguidedlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mguidedlda\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGuidedLDA\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mguidedlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\guidedlda\\guidedlda.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;31m# import lda.utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m## edit 1.9.2020\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lda\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlda\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLDA\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m  \u001b[1;31m# noqa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lda\\lda.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lda\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lda._lda'"
     ]
    }
   ],
   "source": [
    "from guidedlda import GuidedLDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dea50fa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133412, 44872)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0067b47",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GuidedLDA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_860/3229443655.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGuidedLDA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GuidedLDA' is not defined"
     ]
    }
   ],
   "source": [
    "model = GuidedLDA(n_topics=5, n_iter=100, random_state=7, refresh=20)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9b7d73b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: new fashion beauty rt read brand blog week\n",
      "Topic 1: new need beauty blog rt skin read clothes\n",
      "Topic 2: rt health new fashion year need thanks healthy\n",
      "Topic 3: new blog read love beauty rt healthy review\n",
      "Topic 4: rt fashion new health people time help change\n"
     ]
    }
   ],
   "source": [
    "topic_word = model.topic_word_\n",
    "n_top_words = 8\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "4750d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guided LDA with seed topics.\n",
    "seed_topic_list = [['perfume','lipstick','lipsticks','lips','eyes','cheeks', 'fragrance', 'scents', 'skin', 'makeup','lotion', 'beauty', 'perfume', 'perfumery','perfumers', 'beauty','cream','fragrant', 'smell', 'color', 'colour','palette', 'gloss', 'shades','body', 'eyeliner', 'glitter', 'primer', 'concealer','powder','bronzer','eyebrow', 'lip', 'skincare', 'cosmetics','bronzer', 'bath', 'mascara', 'makeover','eyeshadow', 'fragrances', 'balm', 'shades', 'blush', 'nail', 'lacquer', 'shade', 'lash', 'lashes', 'body', 'eyeshadows','palettes', 'oil', 'cologne', 'oils', 'skincare', 'hair', 'scrub', 'scrubs', 'facial', 'serum', 'cleanser', 'serum','pigments', 'shine', 'shade', 'brush', 'foundation','stain', 'mask', 'pore', 'eyebrows', 'liner', 'moisturizer'],\n",
    "                   ['fashion', 'coat', 'textiles', 'footwear', 'clothing', 'cotton', 'costumes', 'collection', 'boutique', 'suit', 'retail', 'sneakers', 'wardrobe', 'streetwear','runway', 'jewellery', 'gala', 'flannels', 'couture', 'bags', 'dress', 'gown', 'retail', 'menswear', 'womenswear', 'shoes', 'casualwear', 'shopping', 'designer', 'denim', 'jeans', 'sportswear', 'clothes', 'athleisure', 'apparel'],\n",
    "                   ['gene', 'genes', 'exercise', 'exercises', 'weight', 'obesity', 'diet', 'fat', 'body', 'carb', 'food', 'protein', 'muscle', 'brain', 'healthy', 'health', 'glymphatic', 'antioxidant', 'alcoholic', 'nonalcoholic', 'liver', 'fatty', 'inflammation', 'toxin', 'toxins', 'undigested', 'recipes', 'meat', 'meats', 'healthier', 'antibiotics', 'hormones', 'metabolism','appetite', 'oxidation', 'physical', 'keto','antioxidants', 'stress', 'healthiest', 'heart', 'blood', 'chronic','carbohydrates', 'metabolic', 'sedentary', 'glycogen', 'therapy', 'ketogenic', 'hiit', 'autophagy', 'sugar', 'calories', 'eat', 'therapeutic', 'ada', 'enzyme', 'addiction', 'mental', 'deficiency', 'nutrient', 'fast', 'meals', 'meal','digestive', 'bloating', 'bowel', 'diarrhea', 'gut', 'microbiome', 'neurotransmitters', 'melatonin', 'mitochondria', 'anxiety', 'tinnitus', 'cancer','insulin', 'probiotic', 'prebiotic', 'organs', 'neuroscience', 'calorie', 'overweight', 'obese', 'diseases', 'systolic', 'diastolic', 'potassium', 'glucose', 'amino', 'recipe', 'pain', 'hormone', 'omega', 'cholesterol', 'endocannabinoid', 'breathing', 'mind', 'relaxed', 'doctors', 'doctor', 'breakfast', 'cognitive','hunger', 'flavonoids', 'concentrate', 'dehydration', 'kidney', 'kidneys', 'microbes', 'bacteria', 'vitamins', 'fiber', 'acetic', 'carbs', 'sugars', 'bran', 'pasta', 'pizza', 'pastries', 'flour', 'rice', 'desserts', 'cereal', 'cereals', 'apple', 'cider', 'vinegar', 'yogurt', 'potatoes', 'chocolate', 'coffee', 'tea', 'teas', 'butter', 'energy', 'thyroid','coconut', 'breath', 'flour', 'peppermint', 'burrito', 'omelette', 'eggs', 'veggies', 'cheese', 'beef', 'chicken', 'pork','milk', 'matcha', 'garlic', 'ketosis', 'nutrients', 'inflammation','dna', 'fats', 'starches', 'foods', 'livers', 'fruit', 'juices', 'diets', 'turmeric', 'pepper', 'telomeres', 'starchy', 'nitrates', 'vitamin', 'beet', 'greens', 'oxalates', 'avocado','carbohydrate', 'microflora', 'cellular', 'proteins', 'cells', 'unhealthy', 'dietary', 'smoking', 'nuts', 'fluoride', 'pregnant', 'supplementation', 'lifestyle', 'glucomannan','salt', 'iron', 'deficient', 'mineral', 'nutrition', 'lunch', 'guacamole', 'poultry', 'smoker', 'celery', 'magnesium', 'joint', 'drinking', 'polyphenols', 'phopholipids', 'astaxanthin','immune', 'isothiocyanate', 'cruciferous', 'choy', 'brussel', 'arugula', 'cauliflower', 'broccoli', 'kale', 'sprouts', 'seeds', 'olive', 'fish', 'cellulite', 'dementia', 'drug', 'digestion', 'depression', 'algae', 'spirulina', 'chlorella', 'animals', 'knee', 'sesame', 'sesamin', 'sesamol', 'arthritis', 'breads', 'crackers', 'sodas', 'burger', 'bun', 'fries', 'tissue', 'flavonoid','cell', 'berries', 'apples', 'artichokes', 'ketones', 'sauerkraut', 'rooibos', 'caffeine', 'bones', 'teeth', 'meditation', 'memory', 'addictions', 'immunity','anticholinergic', 'drugs', 'vegetable', 'alzheimer', 'horseradish', 'mustard', 'wasabi', 'drink', 'dairy', 'diseases', 'parkinson', 'neurons', 'neurodegenerative', 'sulforaphane','glucosinolates', 'peanuts', 'peanut', 'dha', 'epa', 'medications', 'olives', 'allergies', 'glutathione', 'potato', 'salty', 'glycemic', 'sugary', 'supplements', 'snack', 'macadamia', 'pecans', 'walnuts', 'seaweed', 'prosciutto', 'epigenetics', 'diverticulitis', 'kefir', 'triglycerides', 'resveratrol',  'curcumin', 'deficiencies', 'gmo', 'herbicide', 'tuna', 'salmon', 'sardines', 'veggie', 'nut', 'butters', 'grain', 'niacin', 'avocados', 'ketone', 'mindset', 'anthocyanin', 'blueberries','blackberries', 'cabbage', 'fruits', 'alcohol', 'aerobic', 'syrup', 'mushrooms', 'glucan', 'polysaccharide', 'autism', 'glutamate', 'fructose', 'organic', 'ubiquinone', 'cheeses', 'satiety','choline', 'mitochondrial', 'antiviral', 'antifungal', 'antibacterial', 'sinusitis', 'neurogenesis', 'berberine', 'nutraceutical', 'collards', 'microbe', 'neurodegenerative','adiponectin', 'vascular', 'cooking', 'ghee', 'leg', 'strength', 'mass', 'cold', 'flu', 'tortilla', 'chips', 'chip', 'mental', 'rye', 'barley', 'spelt', 'oats', 'casein', 'oleocanthal','enzymes', 'aches', 'pains', 'kimchi', 'cinnamon', 'influenza', 'sclerosis', 'mortality', 'tumors', 'anoxic', 'organoids', 'sweeteners', 'stevia',  'allulose',  'xylitol',  'erythritol',  'glycine', 'pregnancy', 'gestational', 'grains','salad', 'dressings', 'corn', 'soy', 'margarine', 'pancake', 'fatigue', 'headaches', 'lectin','minerals', 'riboflavin', 'migraine', 'carnivore', 'cardiovascular', 'digestible', 'spinach', 'asparagus', 'celery', 'cupcakes', 'cookies', 'ghrelin', 'leptin', 'dopamine', 'serotonin', 'sweets','lipids', 'turkey', 'hummus', 'bacon', 'wine', 'superfoods', 'cbd', 'organ', 'yogurts', 'cigarettes', 'sweet', 'cupcake', 'hyperinsulinemia', 'hormonal','follicles']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "8343e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GuidedLDA(n_topics=3, n_iter=100, random_state=7, refresh=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "51f81d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:lda:n_documents: 133412\n",
      "INFO:lda:vocab_size: 44872\n",
      "INFO:lda:n_words: 1068413\n",
      "INFO:lda:n_topics: 3\n",
      "INFO:lda:n_iter: 100\n",
      "WARNING:lda:all zero row in document-term matrix found\n",
      "INFO:lda:<0> log likelihood: -11458361\n",
      "INFO:lda:<20> log likelihood: -9250896\n",
      "INFO:lda:<40> log likelihood: -9152039\n",
      "INFO:lda:<60> log likelihood: -9079206\n",
      "INFO:lda:<80> log likelihood: -9027341\n",
      "INFO:lda:<99> log likelihood: -8993191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x198d8d26d60>"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id\n",
    "\n",
    "model.fit(X, seed_topics=seed_topics, seed_confidence=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "4900f57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: new blog beauty read makeup review rt today skin post\n",
      "Topic 1: fashion rt new year brand thanks brands business need clothes\n",
      "Topic 2: health healthy new help food time recipe need table diet\n"
     ]
    }
   ],
   "source": [
    "#THIS STEP-Am I not deciding the number of words that determine the specific topic here? Also, I am unclear how they chose THOSE words and how I can make it choose better words\n",
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "5f788023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:lda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top topic: 2 Document: wid\n",
      "top topic: 2 Document: wid\n",
      "top topic: 2 Document: wid\n",
      "top topic: 2 Document: wid\n",
      "top topic: 1 Document: wid\n",
      "top topic: 1 Document: wid\n",
      "top topic: 2 Document: wid\n",
      "top topic: 1 Document: wid\n",
      "top topic: 2 Document: wid\n"
     ]
    }
   ],
   "source": [
    "doc_topic = model.transform(X)\n",
    "for i in range(9):\n",
    "    print(\"top topic: {} Document: {}\".format(doc_topic[i].argmax(),\n",
    "                                          ', '.join(np.array(vocab)[list(reversed(np.argsort(X[i,:])))[0:5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "2c92a35f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.999046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.999806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.998010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000135</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.999193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013938</td>\n",
       "      <td>0.883323</td>\n",
       "      <td>0.102739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2\n",
       "0  0.000394  0.000560  0.999046\n",
       "1  0.000078  0.000116  0.999806\n",
       "2  0.001097  0.000893  0.998010\n",
       "3  0.000135  0.000672  0.999193\n",
       "4  0.013938  0.883323  0.102739"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(doc_topic)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "8ebd0284",
   "metadata": {},
   "outputs": [],
   "source": [
    " df.to_csv('glda_allocation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a0ffa6f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_20084/358137876.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\De\\AppData\\Local\\Temp/ipykernel_20084/358137876.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    \"cosmetics\",\"bronzer\", \"bath\", \"mascara\", \"makeover\",\"eyeshadow\", \"fragrances\", \"balm\", \"shades\", \"blush\", \"nail\", \"lacquer\", \"shade\", \"lash\". \"lashes\", \"body\", \"eyeshadows\",\u001b[0m\n\u001b[1;37m                                                                                                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Beauty_words=[\"perfume\",\"lipstick\",\"lipsticks\",\"lips\",\"eyes\",\"cheeks\", \"fragrance\", \"scents\", \"skin\", \"makeup\",\"lotion\", \"beauty\", \"perfume\", \"perfumery\",\"perfumers\", \"beauty\",\"cream\",\n",
    "              \"fragrant\", \"smell\", \"color\", \"colour\",\"palette\", \"gloss\", \"shades\",\"body\", \"eyeliner\", \"glitter\", \"primer\", \"concealer\",\"powder\",\"bronzer\",\"eyebrow\", \"lip\", \"skincare\", \n",
    "              \"cosmetics\",\"bronzer\", \"bath\", \"mascara\", \"makeover\",\"eyeshadow\", \"fragrances\", \"balm\", \"shades\", \"blush\", \"nail\", \"lacquer\", \"shade\", \"lash\". \"lashes\", \"body\", \"eyeshadows\",\n",
    "              \"palettes\", \"oil\", \"cologne\", \"oils\", \"skincare\", \"hair\", \"scrub\", \"scrubs\", \"facial\", \"serum\", \"cleanser\", \"serum\",\"pigments\", \"shine\", \"shade\", \"brush\", \"foundation\",\n",
    "              \"stain\", \"mask\", \"pore\", \"eyebrows\", \"liner\", \"moisturizer\"]\n",
    "\n",
    "Fashion_words=[\"fashion\", \"coat\", \"textiles\", \"footwear\", \"clothing\", \"cotton\", \"costumes\", \"collection\", \"boutique\", \"suit\", \"retail\", \"sneakers\", \"wardrobe\", \"streetwear\",\"runway\", \n",
    "               \"jewellery\", \"gala\", \"flannels\", \"couture\", \"bags\", \"dress\", \"gown\", \"retail\", \"menswear\", \"womenswear\", \"shoes\", \"casualwear\", \"shopping\", \"designer\", \"denim\", \"jeans\", \n",
    "               \"sportswear\", \"clothes\", \"athleisure\", \"apparel\"]\n",
    "\n",
    "Health_words=[\"gene\", \"genes\", \"exercise\", \"exercises\", \"weight\", \"obesity\", \"diet\", \"fat\", \"body\", \"carb\", \"food\", \"protein\", \"muscle\", \"brain\", \"healthy\", \"health\", \"glymphatic\", \"antioxidant\"\n",
    "              \"alcoholic\", \"nonalcoholic\", \"liver\", \"fatty\", \"inflammation\", \"toxin\", \"toxins\", \"undigested\", \"recipes\", \"meat\", \"meats\", \"healthier\", \"antibiotics\", \"hormones\", \"metabolism\",\n",
    "              \"appetite\", \"oxidation\", \"physical\", \"keto\", \"fasting\", \"antioxidants\", \"stress\", \"healthiest\", \"heart\", \"blood\", \"chronic\", \"aging\", \"carbohydrates\", \"metabolic\", \"sedentary\", \n",
    "              \"glycogen\", \"therapy\", \"ketogenic\", \"hiit\", \"autophagy\", \"sugar\", \"calories\", \"eat\", \"therapeutic\", \"ada\", \"enzyme\", \"addiction\", \"mental\", \"deficiency\", \"nutrient\", \"exercising\",\n",
    "              \"fast\", \"meals\", \"meal\", \"digestive\", \"bloating\", \"bowel\", \"diarrhea\", \"gut\", \"microbiome\", \"neurotransmitters\", \"melatonin\", \"mitochondria\", \"anxiety\", \"tinnitus\", \"cancer\",\n",
    "              \"insulin\", \"eating\", \"probiotic\", \"prebiotic\", \"belly\", \"organs\", \"neuroscience\", \"sleeping\", \"calorie\", \"overweight\", \"obese\", \"diseases\", \"systolic\", \"diastolic\", \"potassium\", \n",
    "              \"glucose\", \"amino\", \"recipe\", \"pain\", \"hormone\", \"omega\", \"cholesterol\", \"endocannabinoid\", \"breathing\", \"mind\", \"relaxed\", \"doctors\", \"doctor\", \"breakfast\", \"wellbeing\", \"cognitive\",\n",
    "              \"hunger\", \"flavonoids\", \"concentrate\", \"dehydration\", \"kidney\", \"kidneys\", \"microbes\", \"bacteria\", \"vitamins\", \"fiber\", \"acetic\", \"carbs\", \"sugars\", \"bran\", \"pasta\", \"pizza\", \n",
    "              \"pastries\", \"flour\", \"rice\", \"desserts\", \"cereal\", \"cereals\", \"apple\", \"cider\", \"vinegar\", \"yogurt\", \"potatoes\", \"chocolate\", \"coffee\", \"tea\", \"teas\", \"butter\", \"energy\", \"thyroid\",\n",
    "              \"coconut\", \"breath\", \"flour\", \"peppermint\", \"burrito\", \"omelette\", \"eggs\", \"veggies\", \"cheese\", \"beef\", \"chicken\", \"pork\",\"milk\", \"matcha\", \"garlic\", \"ketosis\", \"nutrients\", \"inflammation\",\n",
    "              \"dna\", \"fats\", \"starches\", \"foods\", \"livers\", \"fruit\", \"juices\", \"diets\", \"turmeric\", \"pepper\", \"telomeres\", \"starchy\", \"nitrates\", \"vitamin\", \"beet\", \"greens\", \"oxalates\", \"avocado\"\n",
    "              \"carbohydrate\", \"eaten\", \"microflora\", \"cellular\", \"proteins\", \"cells\", \"unhealthy\", \"dietary\", \"smoking\", \"nuts\", \"fluoride\", \"pregnant\", \"supplementation\", \"lifestyle\", \"glucomannan\",\n",
    "              \"salt\", \"iron\", \"deficient\", \"mineral\", \"nutrition\", \"lunch\", \"guacamole\", \"poultry\", \"smoker\", \"celery\", \"magnesium\", \"joint\", \"drinking\", \"polyphenols\", \"phopholipids\", \"astaxanthin\",\n",
    "              \"immune\", \"isothiocyanate\", \"cruciferous\", \"choy\", \"brussel\", \"arugula\", \"cauliflower\", \"broccoli\", \"kale\", \"sprouts\", \"seeds\", \"olive\", \"fish\", \"cellulite\", \"dementia\", \"drug\", \"snacking\",\n",
    "              \"digestion\", \"depression\", \"algae\", \"spirulina\", \"chlorella\", \"animals\", \"knee\", \"sesame\", \"sesamin\", \"sesamol\", \"arthritis\", \"breads\", \"crackers\", \"sodas\", \"burger\", \"bun\", \"fries\", \n",
    "              \"tissue\", \"flavonoid\",\"cell\", \"berries\", \"apples\", \"artichokes\", \"ketones\", \"sauerkraut\", \"rooibos\", \"caffeine\", \"bones\", \"teeth\", \"meditation\", \"memory\", \"addictions\", \"immunity\",\n",
    "              \"anticholinergic\", \"drugs\", \"vegetable\", \"alzheimer\", \"horseradish\", \"mustard\", \"wasabi\", \"drink\", \"dairy\", \"diseases\", \"parkinson\", \"neurons\", \"neurodegenerative\", \"sulforaphane\",\n",
    "              \"glucosinolates\", \"processed\", \"refined\", \"peanuts\", \"peanut\", \"dha\", \"epa\", \"medications\", \"olives\", \"allergies\", \"glutathione\", \"potato\", \"salty\", \"glycemic\", \"sugary\", \"digested\",\n",
    "              \"supplements\", \"snack\", \"macadamia\", \"pecans\", \"walnuts\", \"seaweed\", \"prosciutto\", \"epigenetics\", \"diverticulitis\", \"kefir\", \"triglycerides\", \"resveratrol\",  \"curcumin\", \n",
    "              \"deficiencies\", \"hydrated\", \"gmo\", \"herbicide\", \"tuna\", \"salmon\", \"sardines\", \"veggie\", \"nut\", \"butters\", \"grain\", \"niacin\", \"avocados\", \"ketone\", \"mindset\", \"anthocyanin\", \"blueberries\",\n",
    "              \"blackberries\", \"cabbage\", \"fruits\", \"alcohol\", \"aerobic\", \"syrup\", \"mushrooms\", \"glucan\", \"polysaccharide\", \"autism\", \"glutamate\", \"fructose\", \"organic\", \"ubiquinone\", \"cheeses\", \"satiety\",\n",
    "              \"choline\", \"mitochondrial\", \"detoxified\", \"antiviral\", \"antifungal\", \"antibacterial\", \"sinusitis\", \"neurogenesis\", \"berberine\", \"nutraceutical\", \"collards\", \"microbe\", \"neurodegenerative\",\n",
    "              \"adiponectin\", \"vascular\", \"cooking\", \"ghee\", \"leg\", \"strength\", \"mass\", \"cold\", \"flu\", \"tortilla\", \"chips\", \"chip\", \"mental\", \"rye\", \"barley\", \"spelt\", \"oats\", \"casein\", \"oleocanthal\",\n",
    "              \"enzymes\", \"aches\", \"pains\", \"kimchi\", \"cinnamon\", \"influenza\", \"sclerosis\", \"mortality\", \"tumors\", \"anoxic\", \"organoids\", \"sweeteners\", \"stevia\",  \"allulose\",  \"xylitol\",  \"erythritol\",  \n",
    "              \"glycine\", \"pregnancy\", \"gestational\", \"grains\", \"fried\", \"salad\", \"dressings\", \"corn\", \"soy\", \"margarine\", \"pancake\", \"dehydrated\", \"fatigue\", \"cravings\", \"craving\", \"headaches\", \"lectin\",\n",
    "              \"minerals\", \"riboflavin\", \"migraine\", \"carnivore\", \"cardiovascular\", \"digestible\", \"spinach\", \"asparagus\", \"celery\", \"cupcakes\", \"cookies\", \"ghrelin\", \"leptin\", \"dopamine\", \"serotonin\", \"sweets\",\n",
    "              \"lipids\", \"turkey\", \"hummus\", \"bacon\", \"wine\", \"saturated\", \"superfoods\", \"running\", \"walking\",\"cbd\", \"organ\", \"yogurts\", \"cigarettes\", \"sweet\", \"cupcake\", \"hyperinsulinemia\", \"hormonal\",\"follicles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec976ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - lda\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/win-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/win-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "  - https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "  - https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n"
     ]
    }
   ],
   "source": [
    "conda install lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e1cee90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\de\\anaconda3\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\de\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from gensim) (1.22.4)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\de\\anaconda3\\lib\\site-packages (from gensim) (6.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aa54d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\de\\anaconda3\\lib\\site-packages (1.22.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b36f807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lda in c:\\users\\de\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: pbr<4,>=0.6 in c:\\users\\de\\anaconda3\\lib\\site-packages (from lda) (3.1.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.13.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from lda) (1.22.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27508e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
