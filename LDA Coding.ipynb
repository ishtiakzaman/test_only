{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "391a8722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af585328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNO</th>\n",
       "      <th>Contents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56509</td>\n",
       "      <td>Your favourite perfume can change slightly yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56510</td>\n",
       "      <td>I m giving away Dior s One Essential Skin Boos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56511</td>\n",
       "      <td>Big congrats to   We drew her name this mornin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56512</td>\n",
       "      <td>Do have one or two makeup shades or products y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56513</td>\n",
       "      <td>Just spotted a close up on Cityline of   looki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SNO                                           Contents\n",
       "0  56509  Your favourite perfume can change slightly yea...\n",
       "1  56510  I m giving away Dior s One Essential Skin Boos...\n",
       "2  56511  Big congrats to   We drew her name this mornin...\n",
       "3  56512  Do have one or two makeup shades or products y...\n",
       "4  56513  Just spotted a close up on Cityline of   looki..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "Tweets = pd.read_csv('C:/Users/De/Downloads/Python Files/IM/Beauty_Tweets_Cleaned.csv')\n",
    "\n",
    "# Print head\n",
    "Tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfd6f252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNO</th>\n",
       "      <th>Contents</th>\n",
       "      <th>Contents_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56509</td>\n",
       "      <td>Your favourite perfume can change slightly yea...</td>\n",
       "      <td>your favourite perfume can change slightly yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56510</td>\n",
       "      <td>I m giving away Dior s One Essential Skin Boos...</td>\n",
       "      <td>i m giving away dior s one essential skin boos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56511</td>\n",
       "      <td>Big congrats to   We drew her name this mornin...</td>\n",
       "      <td>big congrats to   we drew her name this mornin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56512</td>\n",
       "      <td>Do have one or two makeup shades or products y...</td>\n",
       "      <td>do have one or two makeup shades or products y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56513</td>\n",
       "      <td>Just spotted a close up on Cityline of   looki...</td>\n",
       "      <td>just spotted a close up on cityline of   looki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     SNO                                           Contents  \\\n",
       "0  56509  Your favourite perfume can change slightly yea...   \n",
       "1  56510  I m giving away Dior s One Essential Skin Boos...   \n",
       "2  56511  Big congrats to   We drew her name this mornin...   \n",
       "3  56512  Do have one or two makeup shades or products y...   \n",
       "4  56513  Just spotted a close up on Cityline of   looki...   \n",
       "\n",
       "                                  Contents_processed  \n",
       "0  your favourite perfume can change slightly yea...  \n",
       "1  i m giving away dior s one essential skin boos...  \n",
       "2  big congrats to   we drew her name this mornin...  \n",
       "3  do have one or two makeup shades or products y...  \n",
       "4  just spotted a close up on cityline of   looki...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# Convert the titles to lowercase\n",
    "Tweets=Tweets.astype(str)\n",
    "Tweets['Contents_processed'] = Tweets['Contents'].map(lambda x: x.lower())\n",
    "# Print head\n",
    "Tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f29829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample only 100 Tweets\n",
    "Tweets = Tweets.sample(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b02b0060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp39-cp39-win_amd64.whl (23.9 MB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from gensim) (1.20.3)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.0.0-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\de\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Collecting Cython==0.29.28\n",
      "  Downloading Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.24\n",
      "    Uninstalling Cython-0.29.24:\n",
      "      Successfully uninstalled Cython-0.29.24\n",
      "Successfully installed Cython-0.29.28 gensim-4.2.0 smart-open-6.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbb9e5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['changing', 'it', 'up', 'can', 'make', 'all', 'the', 'difference', 'giving', 'our', 'bodies', 'little', 'boost', 'and', 'helping', 'us', 'to', 'look', 'and', 'feel', 'our', 'best', 'little', 'changes', 'that', 'will', 'make', 'big', 'difference', 'to']\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data = Tweets.Contents_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a3b36f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad42d5b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\De\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# NLTK Stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3bc9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "285b1159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.3.0-cp39-cp39-win_amd64.whl (11.6 MB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.7-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy) (21.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp39-cp39-win_amd64.whl (112 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.7-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Collecting thinc<8.1.0,>=8.0.14\n",
      "  Downloading thinc-8.0.17-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting wasabi<1.1.0,>=0.9.1\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.3-cp39-cp39-win_amd64.whl (448 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.9\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\de\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\de\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\de\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\de\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\de\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\de\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\de\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\de\\anaconda3\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "  Attempting uninstall: smart-open\n",
      "    Found existing installation: smart-open 6.0.0\n",
      "    Uninstalling smart-open-6.0.0:\n",
      "      Successfully uninstalled smart-open-6.0.0\n",
      "Successfully installed blis-0.7.7 catalogue-2.0.7 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.17 typer-0.4.1 wasabi-0.9.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d49ac13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.26.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.62.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (58.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.7)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.6.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.9.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.20.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\de\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\de\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\de\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\de\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\de\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\de\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\de\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2021.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\de\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\de\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\de\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.3.0\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "278d6309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['change', 'make', 'difference', 'give', 'body', 'little', 'boost', 'help', 'look', 'feel', 'good', 'little', 'change', 'make', 'big', 'difference', 'beauty', 'regime']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8e531fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 2), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 2), (13, 1)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "46632833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63fc0304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.024*\"new\" + 0.024*\"eye\" + 0.013*\"good\" + 0.013*\"get\" + 0.013*\"love\" + '\n",
      "  '0.013*\"gift\" + 0.013*\"life\" + 0.013*\"cute\" + 0.013*\"honestly\" + '\n",
      "  '0.013*\"strong\"'),\n",
      " (1,\n",
      "  '0.049*\"review\" + 0.018*\"find\" + 0.018*\"facial\" + 0.018*\"luxe\" + '\n",
      "  '0.018*\"enhancer\" + 0.018*\"ty\" + 0.018*\"week\" + 0.018*\"rt\" + 0.018*\"cream\" + '\n",
      "  '0.018*\"blog\"'),\n",
      " (2,\n",
      "  '0.017*\"gift\" + 0.017*\"get\" + 0.017*\"ad\" + 0.017*\"beauty\" + 0.017*\"towel\" + '\n",
      "  '0.009*\"easy\" + 0.009*\"know\" + 0.009*\"say\" + 0.009*\"always\" + 0.009*\"give\"'),\n",
      " (3,\n",
      "  '0.023*\"need\" + 0.023*\"twitter\" + 0.012*\"long\" + 0.012*\"keep\" + '\n",
      "  '0.012*\"bottle\" + 0.012*\"back\" + 0.012*\"linger\" + 0.012*\"probably\" + '\n",
      "  '0.012*\"care\" + 0.012*\"bathroom\"'),\n",
      " (4,\n",
      "  '0.023*\"enter\" + 0.017*\"way\" + 0.012*\"love\" + 0.012*\"try\" + 0.012*\"boundary\" '\n",
      "  '+ 0.012*\"push\" + 0.012*\"good\" + 0.012*\"brand\" + 0.012*\"blush\" + '\n",
      "  '0.012*\"kind\"'),\n",
      " (5,\n",
      "  '0.020*\"skin\" + 0.020*\"see\" + 0.020*\"winner\" + 0.010*\"always\" + '\n",
      "  '0.010*\"decide\" + 0.010*\"swatch\" + 0.010*\"go\" + 0.010*\"video\" + '\n",
      "  '0.010*\"hefty\" + 0.010*\"tone\"'),\n",
      " (6,\n",
      "  '0.028*\"year\" + 0.019*\"good\" + 0.019*\"pretty\" + 0.019*\"pyjama\" + '\n",
      "  '0.010*\"well\" + 0.010*\"day\" + 0.010*\"look\" + 0.010*\"post\" + 0.010*\"piece\" + '\n",
      "  '0.010*\"take\"'),\n",
      " (7,\n",
      "  '0.032*\"beauty\" + 0.021*\"come\" + 0.021*\"win\" + 0.021*\"today\" + '\n",
      "  '0.021*\"follow\" + 0.011*\"pack\" + 0.011*\"channel\" + 0.011*\"need\" + '\n",
      "  '0.011*\"lipstick\" + 0.011*\"use\"'),\n",
      " (8,\n",
      "  '0.026*\"blog\" + 0.026*\"beauty\" + 0.026*\"new\" + 0.018*\"good\" + 0.018*\"make\" + '\n",
      "  '0.018*\"little\" + 0.018*\"difference\" + 0.018*\"change\" + 0.018*\"eyeshadow\" + '\n",
      "  '0.018*\"giveaway\"'),\n",
      " (9,\n",
      "  '0.021*\"beauty\" + 0.014*\"read\" + 0.014*\"need\" + 0.014*\"product\" + '\n",
      "  '0.014*\"air\" + 0.014*\"really\" + 0.014*\"blog\" + 0.014*\"fragrance\" + '\n",
      "  '0.014*\"work\" + 0.008*\"brand\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d78151ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence Score:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\De\\anaconda3\\lib\\site-packages\\gensim\\topic_coherence\\direct_confirmation_measure.py:204: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  m_lr_i = np.log(numerator / denominator)\n",
      "C:\\Users\\De\\anaconda3\\lib\\site-packages\\gensim\\topic_coherence\\indirect_confirmation_measure.py:323: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return cv1.T.dot(cv2)[0, 0] / (_magnitude(cv1) * _magnitude(cv2))\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ffe56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copied this from https://github.com/RaRe-Technologies/gensim/issues/3040\n",
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "from gensim.topic_coherence import direct_confirmation_measure\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "ADD_VALUE = 1\n",
    "\n",
    "\n",
    "def custom_log_ratio_measure(segmented_topics, accumulator, normalize=False, with_std=False, with_support=False):\n",
    "    topic_coherences = []\n",
    "    num_docs = float(accumulator.num_docs)\n",
    "    for s_i in segmented_topics:\n",
    "        segment_sims = []\n",
    "        for w_prime, w_star in s_i:\n",
    "            w_prime_count = accumulator[w_prime]\n",
    "            w_star_count = accumulator[w_star]\n",
    "            co_occur_count = accumulator[w_prime, w_star]\n",
    "\n",
    "            if normalize:\n",
    "                # For normalized log ratio measure\n",
    "                numerator = custom_log_ratio_measure([[(w_prime, w_star)]], accumulator)[0]\n",
    "                co_doc_prob = co_occur_count / num_docs\n",
    "                m_lr_i = numerator / (-np.log(co_doc_prob + direct_confirmation_measure.EPSILON))\n",
    "            else:\n",
    "                # For log ratio measure without normalization\n",
    "                ### _custom: Added the following 6 lines, to prevent a division by zero error.\n",
    "                if w_star_count == 0:\n",
    "                    log.info(f\"w_star_count of {w_star} == 0. Adding {ADD_VALUE} to the count to prevent error. \")\n",
    "                    w_star_count += ADD_VALUE\n",
    "                if w_prime_count == 0:\n",
    "                    log.info(f\"w_prime_count of {w_prime} == 0. Adding {ADD_VALUE} to the count to prevent error. \")\n",
    "                    w_prime_count += ADD_VALUE\n",
    "                numerator = (co_occur_count / num_docs) + direct_confirmation_measure.EPSILON\n",
    "                denominator = (w_prime_count / num_docs) * (w_star_count / num_docs)\n",
    "                m_lr_i = np.log(numerator / denominator)\n",
    "\n",
    "            segment_sims.append(m_lr_i)\n",
    "\n",
    "        topic_coherences.append(direct_confirmation_measure.aggregate_segment_sims(segment_sims, with_std, with_support))\n",
    "\n",
    "    return topic_coherences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "715a3a18",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'custom_log_ratio_measure'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5964/461342218.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopic_coherence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdirect_confirmation_measure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcustom_log_ratio_measure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdirect_confirmation_measure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_ratio_measure\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_log_ratio_measure\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'custom_log_ratio_measure'"
     ]
    }
   ],
   "source": [
    "from gensim.topic_coherence import direct_confirmation_measure\n",
    "from my_custom_module import custom_log_ratio_measure\n",
    "\n",
    "direct_confirmation_measure.log_ratio_measure = custom_log_ratio_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e7e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
