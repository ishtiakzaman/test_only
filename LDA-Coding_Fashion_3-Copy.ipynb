{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9e75119",
   "metadata": {},
   "source": [
    "Step 1: Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cf32567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import spacy\n",
    "import nltk\n",
    "import gensim\n",
    "import plotnine\n",
    "import tomotopy\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f432c38f",
   "metadata": {},
   "source": [
    "Step 2: Loading the data\n",
    "Note: The text file was copied into a csv file titled Beauty_Tweers_Cleaned.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af585328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SNO</th>\n",
       "      <th>Contents_Processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307979</td>\n",
       "      <td>. a decent safe home in the uk shouldn t be su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>170959</td>\n",
       "      <td>. a friend and a wonderful mentor has done an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>306481</td>\n",
       "      <td>. a good magazine is a nation talking to itsel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>306762</td>\n",
       "      <td>. a good newspaper i suppose is a nation talki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>306815</td>\n",
       "      <td>. a good newspaper or magazine is a nation tal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      SNO                                 Contents_Processed\n",
       "0  307979  . a decent safe home in the uk shouldn t be su...\n",
       "1  170959  . a friend and a wonderful mentor has done an ...\n",
       "2  306481  . a good magazine is a nation talking to itsel...\n",
       "3  306762  . a good newspaper i suppose is a nation talki...\n",
       "4  306815  . a good newspaper or magazine is a nation tal..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "Tweets = pd.read_csv('C:/Users/De/Downloads/Python Files/IM/Fashion_TweetsForPython.csv')\n",
    "\n",
    "# Print head\n",
    "Tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e9b749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46463"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1981d712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def run_preprocess(tweets, min_token_len=3, rm_accent=True, bigram_min_cnt=5, bigram_thresh=100,\n",
    "                   extra_stops=['from','subject','re', 'edu','use'],\n",
    "                   postags=['NOUN','VERB','ADV','ADJ']):\n",
    "\n",
    "    '''Function wrapper to preprocess the 20Newsgroup dataset and generate ready to model results\n",
    "    \n",
    "    *** Inputs**\n",
    "    news:obj -> 20Newsgroup object from sklearn (i.e. 20fetch...)\n",
    "    min_token_len: int -> tokens less than this number are excluded during tokenization\n",
    "    rm_accent : bool -> flag whether to remove deaccents\n",
    "    bigram_min_cnt: int -> ignore all words and bigrams with total collected count lower than this value\n",
    "    bigram_thresh: int -> threshold for building phrases, higher means fewer phrases\n",
    "    extra_stops: list -> extra stopwords to ignore asidr from NLTK default\n",
    "    postags:list -> words/bigrams to include based on POS (part-of-speech)\n",
    "    \n",
    "    ** Returns**\n",
    "    df: Master df with 20newgroup data and labels\n",
    "    word_list_lemmatized: list -> list of lists w/ lemmatized bigrams \n",
    "    '''\n",
    "    \n",
    "    ### Setting up stopwords and Spacy\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    st_words = stopwords.words('english')\n",
    "    st_words.extend(extra_stops)\n",
    "    \n",
    "    # Build master dataframe\n",
    "    df = pd.Tweets([SNO, Contents_Processed]).T\n",
    "    df = df.set_index(0)\n",
    "\n",
    "    #df = pd.concat([df, pd.Series(news.target_names)],axis=1, join=\"inner\")\n",
    "    #df.reset_index(inplace=True)\n",
    "    #df.columns = [\"topic_id\", \"content\", \"topic_name\"]\n",
    "\n",
    "    # Convert values to list\n",
    "    doc_list = df.cContents_Processed.values.tolist()\n",
    "\n",
    "    # Remove email signs, newlines, single quotes\n",
    "    doc_list = [re.sub(r'\\S*@\\S*\\s?', '', txt) for txt in doc_list]\n",
    "    doc_list = [re.sub(r'\\s+', ' ', txt) for txt in doc_list]\n",
    "    doc_list = [re.sub(r\"\\'\", \"\", txt) for txt in doc_list]\n",
    "\n",
    "    # Tokenize based on min_token_len and deaccent flags\n",
    "    print(\"Tokenizing...\\n\")\n",
    "    word_list = [simple_preprocess(txt, deacc=rm_accent, min_len=min_token_len) for txt in doc_list]\n",
    "     \n",
    "    # Create bigram models\n",
    "    bigram = Phrases(word_list, min_count=bigram_min_cnt, threshold=bigram_thresh) # use original wordlist to build model\n",
    "    bigram_model = Phraser(bigram)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    print(\"Removing Stopwords...\\n\")\n",
    "    word_list_nostops = [[word for word in txt if word not in st_words] for txt in word_list]\n",
    "    \n",
    "    # Implement bigram models\n",
    "    print(\"Create bigrams...\\n\")\n",
    "    word_bigrams = [bigram_model[w_vec] for w_vec in word_list_nostops] # implement it in the list w/ no stopwords\n",
    "    \n",
    "    # Lemmatize POS-tags to keep\n",
    "    print(\"Lemmatizing, keeping \" + \",\".join(postags)+ \" POS tags...\\n\")\n",
    "    word_list_lemmatized = lemmatize(word_bigrams, ptags=postags)\n",
    "\n",
    "    print(\"Done preprocessing \" + str(df.shape[0]) + \" documents\")\n",
    "    return df, word_list_lemmatized\n",
    "    \n",
    "\n",
    "# Helper function    \n",
    "def lemmatize(word_list, ptags):\n",
    "    '''Lemmatizes words based on allowed postags, input format is list of sublists \n",
    "       with strings'''\n",
    "    spC = spacy.load('en_core_web_sm')\n",
    "    lem_lists =[]\n",
    "    for vec in word_list:\n",
    "        sentence = spC(\" \".join(vec))\n",
    "        lem_lists.append([token.lemma_ for token in sentence if token.pos_ in ptags])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "252830cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'Tweets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3788/856622622.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_list_lemmatized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3788/3456123957.py\u001b[0m in \u001b[0;36mrun_preprocess\u001b[1;34m(tweets, min_token_len, rm_accent, bigram_min_cnt, bigram_thresh, extra_stops, postags)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# Build master dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSNO\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mContents_Processed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_SparseArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 261\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"module 'pandas' has no attribute '{name}'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'Tweets'"
     ]
    }
   ],
   "source": [
    "df, word_list_lemmatized = run_preprocess(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c58fa4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp\n",
    "term_weight = tp.TermWeight.ONE\n",
    "hdp = tp.HDPModel(tw=term_weight, min_cf=5, rm_top=7, gamma=1,\n",
    "                  alpha=0.1, initial_k=10, seed=99999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de8da250",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_list_lemmatized' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11460/2981447442.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Add docs to train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mvec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_list_lemmatized\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mhdp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Initiate sampling burn-in  (i.e. discard N first iterations)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_list_lemmatized' is not defined"
     ]
    }
   ],
   "source": [
    "# Add docs to train\n",
    "for vec in word_list_lemmatized:\n",
    "    hdp.add_doc(vec)\n",
    "\n",
    "# Initiate sampling burn-in  (i.e. discard N first iterations)\n",
    "hdp.burn_in = 100\n",
    "hdp.train(0)\n",
    "print('Num docs:', len(hdp.docs), ', Vocab size:', hdp.num_vocabs,\n",
    "      ', Num words:', hdp.num_words)\n",
    "print('Removed top words:', hdp.removed_top_words)\n",
    "\n",
    "# Train model\n",
    "for i in range(0, 1000, 100):\n",
    "    hdp.train(100) # 100 iterations at a time\n",
    "    print('Iteration: {}\\tLog-likelihood: {}\\tNum. of topics: {}'.format(i, hdp.ll_per_word, hdp.live_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d26f1c3",
   "metadata": {},
   "source": [
    "Step 3: Phrase Modeling: Bigram and Trigram Models\n",
    "Bigrams are two words frequently occurring together in the document. Trigrams are 3 words frequently occurring. Some examples in our example are: 'back_bumper', 'oil_leakage', 'maryland_college_park' etc.\n",
    "\n",
    "Gensim's Phrases model can build and implement the bigrams, trigrams, quadgrams and more. The two important arguments to Phrases are min_count and threshold."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
